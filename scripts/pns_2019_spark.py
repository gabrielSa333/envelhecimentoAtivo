#!/usr/bin/env python3
"""
ETL PNS 2019 - VersÃ£o PySpark Final
====================================
Baseado na versÃ£o Pandas funcionando, otimizado para processamento distribuÃ­do.
MantÃ©m a mesma lÃ³gica de extraÃ§Ã£o, transformaÃ§Ã£o e derivaÃ§Ã£o de variÃ¡veis.
"""

import os
import sys
import re
from pathlib import Path
import pandas as pd
import numpy as np

# ==========================================
# CONFIGURAÃ‡Ã•ES
# ==========================================

BASE_PATH = Path("c:/Users/gafeb/Downloads/PNS_2019_20220525")
RAW_FILE = BASE_PATH / "data" / "raw" / "PNS_2019.txt"
SAS_FILE = BASE_PATH / "data" / "raw" / "input_PNS_2019.sas"
OUTPUT_DIR = BASE_PATH / "pns_2019_processado"
OUTPUT_CSV = OUTPUT_DIR / "pns_2019_final_completo.csv"

# Criar diretÃ³rio de saÃ­da
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ==========================================
# COLUNAS DESEJADAS (do cÃ³digo Pandas)
# ==========================================

DESIRED_COLUMNS = {
    "V0001": "uf",
    "V00291": "peso_amostral",
    "UPA_PNS": "upa",
    "V0024": "estrato",
    "V0015": "id_domicilio",
    "V0028": "id_individuo",
    "V0029": "area_urbana",
    "V0029A": "cod_mun_ibge",
    "V0031": "area_metropolitana",
    "V00293": "regiao",
    "C006": "sexo",
    "C008": "idade",
    "C009": "raca_cor",
    "D00901": "anos_estudo",
    "E01602": "renda_percapita",
    "C011": "situacao_ocupacional",
    "C004": "mora_sozinho",
    "V0026": "num_pessoas_domicilio",
    "P00102": "autoavaliacao_saude",
    "Q00201": "hipertensao",
    "Q03001": "diabetes",
    "Q060": "doenca_cardiaca",
    "Q06207": "avc",
    "Q092": "doenca_respiratoria",
    "Q11201": "cancer",
    "N001": "depressao_diag",
    "I00101": "possui_plano_saude",
    "J001": "consulta_12m",
    "J01101": "internacoes_12m",
    "O00101": "vacina_influenza",
    "R002010": "num_medicamentos",
    "P034": "atividade_fisica",
    "P050": "fumante_atual",
    "M001": "usa_internet",
    "M011011": "usa_celular",
    "K004": "dificuldade_banho",
    "K010": "dificuldade_vestir",
    "K001": "dificuldade_alimentar",
    "K01901": "ajuda_adl",
    "K022": "dificuldade_compras",
    "K031": "dificuldade_medico",
    "K03401": "ajuda_iadl",
    "K05401": "queda_12m",
    "J026": "atendimento_sus",
    "P00103": "peso_real",
    "P00402": "altura",
}

# CorreÃ§Ãµes de posiÃ§Ãµes (do cÃ³digo Pandas)
POSITION_CORRECTIONS = {
    "P00402": (602, 3),   # altura: 3 dÃ­gitos
    "V0029": (1381, 1),   # area_urbana: 1 dÃ­gito
    "V00293": (1503, 1),  # regiao: 1 dÃ­gito
}

# ==========================================
# MAPEAMENTOS CATEGÃ“RICOS (do cÃ³digo Pandas)
# ==========================================

MAPPINGS = {
    "uf": {
        "11": "RondÃ´nia", "12": "Acre", "13": "Amazonas", "14": "Roraima", 
        "15": "ParÃ¡", "16": "AmapÃ¡", "17": "Tocantins",
        "21": "MaranhÃ£o", "22": "PiauÃ­", "23": "CearÃ¡", "24": "Rio Grande do Norte", 
        "25": "ParaÃ­ba", "26": "Pernambuco", "27": "Alagoas", "28": "Sergipe", "29": "Bahia",
        "31": "Minas Gerais", "32": "EspÃ­rito Santo", "33": "Rio de Janeiro", "35": "SÃ£o Paulo",
        "41": "ParanÃ¡", "42": "Santa Catarina", "43": "Rio Grande do Sul",
        "50": "Mato Grosso do Sul", "51": "Mato Grosso", "52": "GoiÃ¡s", "53": "Distrito Federal"
    },
    "sexo": {"1": "Masculino", "2": "Feminino"},
    "raca_cor": {"1": "Branca", "2": "Preta", "3": "Amarela", "4": "Parda", "5": "IndÃ­gena", "9": "Ignorado"},
    "situacao_ocupacional": {"1": "Ocupado", "2": "Desocupado", "3": "Fora da forÃ§a de trabalho", "4": "NÃ£o aplicÃ¡vel", "9": "Ignorado"},
    "area_urbana": {"1": "Urbano", "2": "Rural"},
    "regiao": {"1": "Norte", "2": "Nordeste", "3": "Sudeste", "4": "Sul", "5": "Centro-Oeste"},
    "autoavaliacao_saude": {"1": "Muito boa", "2": "Boa", "3": "Regular", "4": "Ruim", "5": "Muito ruim"},
    "hipertensao": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "9": "Ignorado"},
    "diabetes": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "9": "Ignorado"},
    "doenca_cardiaca": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "9": "Ignorado"},
    "avc": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "9": "Ignorado"},
    "doenca_respiratoria": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "9": "Ignorado"},
    "cancer": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "3": "NÃ£o sabe", "9": "Ignorado"},
    "depressao_diag": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "9": "Ignorado"},
    "possui_plano_saude": {"1": "Sim", "2": "NÃ£o", "9": "Ignorado"},
    "consulta_12m": {"1": "Sim", "2": "NÃ£o", "3": "NÃ£o se aplica", "9": "Ignorado"},
    "atendimento_sus": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o", "3": "NÃ£o sabe"},
    "vacina_influenza": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "9": "Ignorado"},
    "atividade_fisica": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o sabe", "9": "Ignorado"},
    "fumante_atual": {"1": "Sim", "2": "NÃ£o", "3": "NÃ£o, mas jÃ¡ fumou", "0": "NÃ£o", "9": "Ignorado"},
    "usa_internet": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o", "3": "NÃ£o sabe", "9": "Ignorado"},
    "usa_celular": {"1": "Sim", "2": "NÃ£o", "0": "NÃ£o", "9": "Ignorado"},
    "dificuldade_alimentar": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "NÃ£o consegue", "9": "Ignorado"},
    "dificuldade_banho": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "NÃ£o consegue", "9": "Ignorado"},
    "dificuldade_vestir": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "NÃ£o consegue", "9": "Ignorado"},
    "dificuldade_compras": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "NÃ£o consegue", "9": "Ignorado"},
    "dificuldade_medico": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "NÃ£o consegue", "9": "Ignorado"},
    "queda_12m": {"1": "Sim", "2": "NÃ£o", "9": "Ignorado"},
    "ajuda_adl": {"1": "Sim", "2": "NÃ£o", "9": "Ignorado"},
    "ajuda_iadl": {"1": "Sim", "2": "NÃ£o", "9": "Ignorado"},
}

# ==========================================
# SETUP SPARK
# ==========================================

print("="*70)
print("ğŸš€ ETL PNS 2019 - VERSÃƒO PYSPARK FINAL")
print("="*70)

# Configurar ambiente Windows
os.environ['HADOOP_HOME'] = ''
os.environ['hadoop.home.dir'] = ''
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# Imports PySpark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, substring, when, lit, trim, regexp_replace, length as spark_length
from pyspark.sql.types import DoubleType, IntegerType, StringType
import findspark

findspark.init()

# Criar Spark
spark = SparkSession.builder \
    .appName("ETL_PNS_2019_Final") \
    .master("local[*]") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.memory", "8g") \
    .config("spark.sql.shuffle.partitions", "32") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")
print(f"âœ… Spark {spark.version} iniciado\n")

# ==========================================
# FUNÃ‡Ã•ES AUXILIARES
# ==========================================

def parse_sas_positions(sas_file):
    """Parse SAS file para obter posiÃ§Ãµes (0-based)"""
    positions = {}
    pattern = re.compile(r'@(\d{5})\s*(\w+)\s*(\$?\d+(?:\.\d*)?)')
    
    with open(sas_file, 'r', encoding='latin-1') as f:
        for line in f:
            match = pattern.search(line)
            if match:
                pos = int(match.group(1)) - 1  # 0-based como no Pandas
                code = match.group(2)
                length_str = match.group(3).replace('$', '').split('.')[0]
                length = int(length_str)
                positions[code] = (pos, length)
    
    # Aplicar correÃ§Ãµes
    positions.update(POSITION_CORRECTIONS)
    
    return positions

def clean_and_map_column(df, col_name, mapping):
    """
    Limpa coluna (remove zeros Ã  esquerda, trim) e aplica mapeamento.
    Replica lÃ³gica do Pandas: .strip().lstrip('0').replace('', '0')
    """
    # Trim
    df = df.withColumn(col_name, trim(col(col_name)))
    
    # Remover zeros Ã  esquerda (mas manter '0' se for sÃ³ zeros)
    df = df.withColumn(
        col_name,
        when(regexp_replace(col(col_name), "^0+", "") == "", "0")
        .otherwise(regexp_replace(col(col_name), "^0+", ""))
    )
    
    # Aplicar mapeamento
    mapping_expr = col(col_name)
    for key, value in mapping.items():
        mapping_expr = when(col(col_name) == key, value).otherwise(mapping_expr)
    
    df = df.withColumn(col_name, mapping_expr)
    return df

# ==========================================
# ETAPA 1: PARSE POSIÃ‡Ã•ES
# ==========================================

print("ğŸ“– [1/6] Parseando posiÃ§Ãµes do SAS...")
positions = parse_sas_positions(SAS_FILE)

print(f"âœ… {len(positions)} posiÃ§Ãµes encontradas\n")
print("ğŸ“‹ PosiÃ§Ãµes para colunas desejadas:")
for code in list(DESIRED_COLUMNS.keys())[:10]:
    if code in positions:
        print(f"   {code:15} â†’ pos={positions[code][0]:4}, len={positions[code][1]}")
print(f"   ... e mais {len(DESIRED_COLUMNS) - 10}")

# ==========================================
# ETAPA 2: EXTRAÃ‡ÃƒO COM SPARK
# ==========================================

print("\nğŸ“¥ [2/6] Extraindo colunas do arquivo raw...")

# Ler arquivo como texto
df_raw = spark.read.text(str(RAW_FILE), lineSep="\n")
n_total = df_raw.count()
print(f"   âœ“ {n_total:,} registros lidos")

# Extrair apenas colunas desejadas
extract_expr = []
codes_found = []

for code, col_name in DESIRED_COLUMNS.items():
    if code in positions:
        pos, length = positions[code]
        # Substring Ã© 1-based no Spark, entÃ£o pos+1
        extract_expr.append(substring(col("value"), pos + 1, length).alias(col_name))
        codes_found.append(code)
    else:
        print(f"   âš ï¸  {code} nÃ£o encontrado no SAS")

df_extracted = df_raw.select(*extract_expr)
print(f"   âœ“ {len(extract_expr)} colunas extraÃ­das")

# ==========================================
# ETAPA 3: CONVERSÃƒO DE TIPOS
# ==========================================

print("\nğŸ”¢ [3/6] Convertendo tipos de dados...")

# Colunas numÃ©ricas
numeric_cols = [
    "peso_amostral", "idade", "anos_estudo", "renda_percapita", 
    "num_medicamentos", "peso_real", "altura", "autoavaliacao_saude",
    "internacoes_12m"
]

# Converter numÃ©ricas
for col_name in numeric_cols:
    if col_name in df_extracted.columns:
        df_extracted = df_extracted.withColumn(
            col_name,
            when(
                trim(col(col_name)).rlike("^[0-9]+$") & (trim(col(col_name)) != ""),
                col(col_name).cast(DoubleType())
            ).otherwise(lit(None))
        )

# Trim em todas as strings
string_cols = [c for c in df_extracted.columns if c not in numeric_cols]
for col_name in string_cols:
    df_extracted = df_extracted.withColumn(col_name, trim(col(col_name)))

print(f"   âœ“ {len(numeric_cols)} colunas numÃ©ricas convertidas")

# ==========================================
# ETAPA 4: FILTRO 60+ ANOS
# ==========================================

print("\nğŸ¯ [4/6] Filtrando populaÃ§Ã£o 60+ anos...")
df_filtered = df_extracted.filter(col("idade") >= 60)
n_60plus = df_filtered.count()
print(f"   âœ“ {n_total:,} â†’ {n_60plus:,} registros ({(n_60plus/n_total)*100:.1f}%)")

# ==========================================
# ETAPA 5: MAPEAMENTOS CATEGÃ“RICOS
# ==========================================

print("\nğŸ·ï¸  [5/6] Aplicando mapeamentos categÃ³ricos...")

mapped_count = 0
for col_name, mapping in MAPPINGS.items():
    if col_name in df_filtered.columns:
        print(f"   Mapeando {col_name}...", end="")
        df_filtered = clean_and_map_column(df_filtered, col_name, mapping)
        mapped_count += 1
        print(" âœ“")

print(f"   âœ“ {mapped_count} colunas mapeadas")

# ==========================================
# ETAPA 6: VARIÃVEIS DERIVADAS
# ==========================================

print("\nğŸ§® [6/6] Criando variÃ¡veis derivadas...")

# Converter para Pandas para operaÃ§Ãµes complexas
print("   Convertendo para Pandas...")
df_pandas = df_filtered.toPandas()

# 1. IMC
print("   [a] Calculando IMC...")
if "peso_real" in df_pandas.columns and "altura" in df_pandas.columns:
    df_pandas["altura_m"] = df_pandas["altura"] / 100  # cm para metros
    df_pandas["imc"] = df_pandas["peso_real"] / (df_pandas["altura_m"] ** 2)
    df_pandas["imc"] = df_pandas["imc"].fillna(df_pandas["imc"].median())
    print(f"      âœ“ IMC calculado (mÃ©dia: {df_pandas['imc'].mean():.2f})")

# 2. Converter binÃ¡rios para 0/1
print("   [b] Convertendo variÃ¡veis binÃ¡rias...")
binary_cols = [
    "possui_plano_saude", "consulta_12m", "atendimento_sus", "usa_internet",
    "usa_celular", "depressao_diag", "vacina_influenza", "atividade_fisica",
    "fumante_atual", "hipertensao", "diabetes", "doenca_cardiaca", "avc",
    "doenca_respiratoria", "cancer"
]

for col_name in binary_cols:
    if col_name in df_pandas.columns:
        # Converter 'Sim'/'NÃ£o' para 1/0
        df_pandas[col_name] = df_pandas[col_name].replace({"Sim": 1, "NÃ£o": 0, "NÃ£o sabe": 0, "Ignorado": 0})
        df_pandas[col_name] = pd.to_numeric(df_pandas[col_name], errors='coerce').fillna(0).astype(int)

# 3. Multimorbidade
print("   [c] Calculando multimorbidade...")
chronic_cols = ["hipertensao", "diabetes", "doenca_cardiaca", "avc", "doenca_respiratoria", "cancer", "depressao_diag"]
chronic_present = [c for c in chronic_cols if c in df_pandas.columns]

if chronic_present:
    df_pandas["multimorbidade_count"] = df_pandas[chronic_present].sum(axis=1)
    df_pandas["multimorbidade_cat"] = pd.cut(
        df_pandas["multimorbidade_count"],
        bins=[-1, 0, 1, 2, 100],
        labels=["0", "1", "2", "3+"]
    ).astype(str)
    print(f"      âœ“ Multimorbidade (mÃ©dia: {df_pandas['multimorbidade_count'].mean():.2f})")

# 4. Escores funcionais (ADL/IADL)
print("   [d] Calculando escores funcionais...")
adl_cols = ["dificuldade_vestir", "dificuldade_banho", "dificuldade_alimentar"]
iadl_cols = ["dificuldade_compras", "dificuldade_medico"]

# Converter dificuldades para numÃ©rico (Nenhuma=0, Alguma=1, Muita=2, NÃ£o consegue=3)
difficulty_mapping = {"Nenhuma": 0, "Alguma": 1, "Muita": 2, "NÃ£o consegue": 3, "Ignorado": 0}

for col_name in adl_cols + iadl_cols:
    if col_name in df_pandas.columns:
        df_pandas[col_name] = df_pandas[col_name].replace(difficulty_mapping)
        df_pandas[col_name] = pd.to_numeric(df_pandas[col_name], errors='coerce').fillna(0).astype(int)

adl_present = [c for c in adl_cols if c in df_pandas.columns]
iadl_present = [c for c in iadl_cols if c in df_pandas.columns]

if adl_present:
    df_pandas["adl_score"] = df_pandas[adl_present].sum(axis=1)
    print(f"      âœ“ ADL Score (mÃ©dia: {df_pandas['adl_score'].mean():.2f})")

if iadl_present:
    df_pandas["iadl_score"] = df_pandas[iadl_present].sum(axis=1)
    print(f"      âœ“ IADL Score (mÃ©dia: {df_pandas['iadl_score'].mean():.2f})")

# 5. Functional Score normalizado
if adl_present or iadl_present:
    df_pandas["functional_raw"] = 0
    if "adl_score" in df_pandas.columns:
        df_pandas["functional_raw"] += df_pandas["adl_score"]
    if "iadl_score" in df_pandas.columns:
        df_pandas["functional_raw"] += df_pandas["iadl_score"]
    
    max_raw = df_pandas["functional_raw"].max() if df_pandas["functional_raw"].max() > 0 else 1
    df_pandas["functional_score"] = 1 - (df_pandas["functional_raw"] / max_raw)
    print(f"      âœ“ Functional Score (mÃ©dia: {df_pandas['functional_score'].mean():.3f})")

# 6. DependÃªncia SUS
print("   [e] Calculando dependÃªncia SUS...")
if "possui_plano_saude" in df_pandas.columns and "atendimento_sus" in df_pandas.columns:
    df_pandas["dependencia_SUS"] = (
        (df_pandas["possui_plano_saude"] == 0) & (df_pandas["atendimento_sus"] == 1)
    ).astype(int)
    print(f"      âœ“ DependÃªncia SUS ({df_pandas['dependencia_SUS'].sum():,} pessoas, {df_pandas['dependencia_SUS'].mean()*100:.1f}%)")

# 7. Cobertura influenza
if "vacina_influenza" in df_pandas.columns:
    df_pandas["cobertura_influenza"] = df_pandas["vacina_influenza"]

# 8. Health Score composto
print("   [f] Calculando Health Score...")
required_cols = ["autoavaliacao_saude", "multimorbidade_count", "functional_score"]
if all(c in df_pandas.columns for c in required_cols):
    # Converter autoavaliaÃ§Ã£o para numÃ©rico
    health_mapping = {"Muito boa": 1, "Boa": 2, "Regular": 3, "Ruim": 4, "Muito ruim": 5}
    df_pandas["autoav_numeric"] = df_pandas["autoavaliacao_saude"].replace(health_mapping)
    df_pandas["autoav_numeric"] = pd.to_numeric(df_pandas["autoav_numeric"], errors='coerce')
    
    # Z-scores
    df_pandas["autoav_z"] = (df_pandas["autoav_numeric"] - df_pandas["autoav_numeric"].mean()) / df_pandas["autoav_numeric"].std(ddof=0)
    df_pandas["multimorb_z"] = (df_pandas["multimorbidade_count"] - df_pandas["multimorbidade_count"].mean()) / df_pandas["multimorbidade_count"].std(ddof=0)
    df_pandas["functional_z"] = (df_pandas["functional_score"] - df_pandas["functional_score"].mean()) / df_pandas["functional_score"].std(ddof=0)
    
    # Score composto
    df_pandas["health_score_raw"] = (
        (-0.5 * df_pandas["autoav_z"]) +
        (-0.7 * df_pandas["multimorb_z"]) +
        (1.2 * df_pandas["functional_z"])
    )
    
    # Normalizar 0-1
    min_val = df_pandas["health_score_raw"].min()
    max_val = df_pandas["health_score_raw"].max()
    df_pandas["health_score"] = (df_pandas["health_score_raw"] - min_val) / (max_val - min_val + 1e-9)
    print(f"      âœ“ Health Score (mÃ©dia: {df_pandas['health_score'].mean():.3f})")

# 9. ImputaÃ§Ã£o
print("   [g] Imputando valores faltantes...")
impute_cols = ["num_medicamentos", "idade", "anos_estudo", "renda_percapita"]
for col_name in impute_cols:
    if col_name in df_pandas.columns:
        missing = df_pandas[col_name].isnull().sum()
        if missing > 0:
            df_pandas[col_name] = df_pandas[col_name].fillna(df_pandas[col_name].median())
            print(f"      â€¢ {col_name}: {missing:,} valores imputados")

print("\nâœ… Todas as variÃ¡veis derivadas criadas!")

# ==========================================
# EXPORTAÃ‡ÃƒO FINAL
# ==========================================

print("\n" + "="*70)
print("ğŸ’¾ EXPORTAÃ‡ÃƒO FINAL")
print("="*70)

# Salvar CSV
print(f"\nğŸ“ Salvando {OUTPUT_CSV.name}...")
df_pandas.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')
file_size = OUTPUT_CSV.stat().st_size / 1024**2

print(f"âœ… Arquivo salvo com sucesso!")
print(f"   ğŸ“ Caminho: {OUTPUT_CSV}")
print(f"   ğŸ“Š Registros: {len(df_pandas):,}")
print(f"   ğŸ“‹ Colunas: {len(df_pandas.columns)}")
print(f"   ğŸ’¾ Tamanho: {file_size:.1f} MB")

# Salvar metadados
metadata_file = OUTPUT_DIR / "metadados_completo.txt"
with open(metadata_file, 'w', encoding='utf-8') as f:
    f.write("="*70 + "\n")
    f.write("PNS 2019 - DATASET COMPLETO COM VARIÃVEIS DERIVADAS\n")
    f.write("="*70 + "\n\n")
    f.write(f"Registros: {len(df_pandas):,}\n")
    f.write(f"Colunas: {len(df_pandas.columns)}\n\n")
    f.write("COLUNAS:\n")
    f.write("-"*70 + "\n")
    for col in df_pandas.columns:
        f.write(f"{col:35} {df_pandas[col].dtype}\n")

print(f"\nğŸ“‹ Metadados salvos: {metadata_file.name}")

# Resumo estatÃ­stico
print("\nğŸ“Š RESUMO ESTATÃSTICO:")
print("-"*70)

summary_vars = {
    "Idade": "idade",
    "Sexo": "sexo",
    "Multimorbidade": "multimorbidade_count",
    "Functional Score": "functional_score",
    "Health Score": "health_score",
    "DependÃªncia SUS": "dependencia_SUS"
}

for label, var in summary_vars.items():
    if var in df_pandas.columns:
        if df_pandas[var].dtype in ['object', 'category']:
            top_value = df_pandas[var].value_counts().head(1)
            if len(top_value) > 0:
                print(f"{label:20} (categÃ³rica) - Mais comum: {top_value.index[0]} ({top_value.values[0]:,})")
        else:
            mean_val = df_pandas[var].mean()
            median_val = df_pandas[var].median()
            print(f"{label:20} (numÃ©rica)    - MÃ©dia: {mean_val:.2f}, Mediana: {median_val:.2f}")

print("\n" + "="*70)
print("ğŸ‰ ETL CONCLUÃDO COM SUCESSO!")
print("="*70)
print("\nâœ¨ Dataset pronto para anÃ¡lise!")

# Encerrar Spark
spark.stop()
print("\nğŸ”Œ Spark encerrado")