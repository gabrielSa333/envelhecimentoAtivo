#!/usr/bin/env python3
"""
ETL PNS 2019 - Vers√£o PySpark Final
====================================
Baseado na vers√£o Pandas funcionando, otimizado para processamento distribu√≠do.
Mant√©m a mesma l√≥gica de extra√ß√£o, transforma√ß√£o e deriva√ß√£o de vari√°veis.
"""

import os
import sys
import re
from pathlib import Path
import pandas as pd
import numpy as np

# ==========================================
# CONFIGURA√á√ïES
# ==========================================

BASE_PATH = Path("c:/Users/gafeb/Downloads/PNS_2019_20220525")
RAW_FILE = BASE_PATH / "data" / "raw" / "PNS_2019.txt"
SAS_FILE = BASE_PATH / "data" / "raw" / "input_PNS_2019.sas"
OUTPUT_DIR = BASE_PATH / "pns_2019_processado"
OUTPUT_CSV = OUTPUT_DIR / "pns_2019_final_completo.csv"

# Criar diret√≥rio de sa√≠da
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ==========================================
# COLUNAS DESEJADAS (do c√≥digo Pandas)
# ==========================================

DESIRED_COLUMNS = {
    "V0001": "uf",
    "V00291": "peso_amostral",
    "UPA_PNS": "upa",
    "V0024": "estrato",
    "V0015": "id_domicilio",
    "V0028": "id_individuo",
    "V0029": "area_urbana",
    "V0029A": "cod_mun_ibge",
    "V0031": "area_metropolitana",
    "V00293": "regiao",
    "C006": "sexo",
    "C008": "idade",
    "C009": "raca_cor",
    "D00901": "anos_estudo",
    "E01602": "renda_percapita",
    "C011": "situacao_ocupacional",
    "C004": "mora_sozinho",
    "V0026": "num_pessoas_domicilio",
    "P00102": "autoavaliacao_saude",
    "Q00201": "hipertensao",
    "Q03001": "diabetes",
    "Q060": "doenca_cardiaca",
    "Q06207": "avc",
    "Q092": "doenca_respiratoria",
    "Q11201": "cancer",
    "N001": "depressao_diag",
    "I00101": "possui_plano_saude",
    "J001": "consulta_12m",
    "J01101": "internacoes_12m",
    "O00101": "vacina_influenza",
    "R002010": "num_medicamentos",
    "P034": "atividade_fisica",
    "P050": "fumante_atual",
    "M001": "usa_internet",
    "M011011": "usa_celular",
    "K004": "dificuldade_banho",
    "K010": "dificuldade_vestir",
    "K001": "dificuldade_alimentar",
    "K01901": "ajuda_adl",
    "K022": "dificuldade_compras",
    "K031": "dificuldade_medico",
    "K03401": "ajuda_iadl",
    "K05401": "queda_12m",
    "J026": "atendimento_sus",
    "P00103": "peso_real",
    "P00402": "altura",
}

# Corre√ß√µes de posi√ß√µes (do c√≥digo Pandas)
POSITION_CORRECTIONS = {
    "P00402": (602, 3),   # altura: 3 d√≠gitos
    "V0029": (1381, 1),   # area_urbana: 1 d√≠gito
    "V00293": (1503, 1),  # regiao: 1 d√≠gito
}

# ==========================================
# MAPEAMENTOS CATEG√ìRICOS (do c√≥digo Pandas)
# ==========================================

MAPPINGS = {
    "uf": {
        "11": "Rond√¥nia", "12": "Acre", "13": "Amazonas", "14": "Roraima", 
        "15": "Par√°", "16": "Amap√°", "17": "Tocantins",
        "21": "Maranh√£o", "22": "Piau√≠", "23": "Cear√°", "24": "Rio Grande do Norte", 
        "25": "Para√≠ba", "26": "Pernambuco", "27": "Alagoas", "28": "Sergipe", "29": "Bahia",
        "31": "Minas Gerais", "32": "Esp√≠rito Santo", "33": "Rio de Janeiro", "35": "S√£o Paulo",
        "41": "Paran√°", "42": "Santa Catarina", "43": "Rio Grande do Sul",
        "50": "Mato Grosso do Sul", "51": "Mato Grosso", "52": "Goi√°s", "53": "Distrito Federal"
    },
    "sexo": {"1": "Masculino", "2": "Feminino"},
    "raca_cor": {"1": "Branca", "2": "Preta", "3": "Amarela", "4": "Parda", "5": "Ind√≠gena", "9": "Ignorado"},
    "situacao_ocupacional": {"1": "Ocupado", "2": "Desocupado", "3": "Fora da for√ßa de trabalho", "4": "N√£o aplic√°vel", "9": "Ignorado"},
    "area_urbana": {"1": "Urbano", "2": "Rural"},
    "regiao": {"1": "Norte", "2": "Nordeste", "3": "Sudeste", "4": "Sul", "5": "Centro-Oeste"},
    "autoavaliacao_saude": {"1": "Muito boa", "2": "Boa", "3": "Regular", "4": "Ruim", "5": "Muito ruim"},
    "hipertensao": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "9": "Ignorado"},
    "diabetes": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "9": "Ignorado"},
    "doenca_cardiaca": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "9": "Ignorado"},
    "avc": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "9": "Ignorado"},
    "doenca_respiratoria": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "9": "Ignorado"},
    "cancer": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "3": "N√£o sabe", "9": "Ignorado"},
    "depressao_diag": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "9": "Ignorado"},
    "possui_plano_saude": {"1": "Sim", "2": "N√£o", "9": "Ignorado"},
    "consulta_12m": {"1": "Sim", "2": "N√£o", "3": "N√£o se aplica", "9": "Ignorado"},
    "atendimento_sus": {"1": "Sim", "2": "N√£o", "0": "N√£o", "3": "N√£o sabe"},
    "vacina_influenza": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "9": "Ignorado"},
    "atividade_fisica": {"1": "Sim", "2": "N√£o", "0": "N√£o sabe", "9": "Ignorado"},
    "fumante_atual": {"1": "Sim", "2": "N√£o", "3": "N√£o, mas j√° fumou", "0": "N√£o", "9": "Ignorado"},
    "usa_internet": {"1": "Sim", "2": "N√£o", "0": "N√£o", "3": "N√£o sabe", "9": "Ignorado"},
    "usa_celular": {"1": "Sim", "2": "N√£o", "0": "N√£o", "9": "Ignorado"},
    "dificuldade_alimentar": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "N√£o consegue", "9": "Ignorado"},
    "dificuldade_banho": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "N√£o consegue", "9": "Ignorado"},
    "dificuldade_vestir": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "N√£o consegue", "9": "Ignorado"},
    "dificuldade_compras": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "N√£o consegue", "9": "Ignorado"},
    "dificuldade_medico": {"1": "Nenhuma", "2": "Alguma", "3": "Muita", "4": "N√£o consegue", "9": "Ignorado"},
    "queda_12m": {"1": "Sim", "2": "N√£o", "9": "Ignorado"},
    "ajuda_adl": {"1": "Sim", "2": "N√£o", "9": "Ignorado"},
    "ajuda_iadl": {"1": "Sim", "2": "N√£o", "9": "Ignorado"},
}

# ==========================================
# SETUP SPARK
# ==========================================

print("="*70)
print("üöÄ ETL PNS 2019 - VERS√ÉO PYSPARK FINAL")
print("="*70)

# Configurar ambiente Windows
os.environ['HADOOP_HOME'] = ''
os.environ['hadoop.home.dir'] = ''
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

# Imports PySpark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, substring, when, lit, trim, regexp_replace, length as spark_length
from pyspark.sql.types import DoubleType, IntegerType, StringType
import findspark

findspark.init()

# Criar Spark
spark = SparkSession.builder \
    .appName("ETL_PNS_2019_Final") \
    .master("local[*]") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.memory", "8g") \
    .config("spark.sql.shuffle.partitions", "32") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")
print(f"‚úÖ Spark {spark.version} iniciado\n")

# ==========================================
# FUN√á√ïES AUXILIARES
# ==========================================

def parse_sas_positions(sas_file):
    """Parse SAS file para obter posi√ß√µes (0-based)"""
    positions = {}
    pattern = re.compile(r'@(\d{5})\s*(\w+)\s*(\$?\d+(?:\.\d*)?)')
    
    with open(sas_file, 'r', encoding='latin-1') as f:
        for line in f:
            match = pattern.search(line)
            if match:
                pos = int(match.group(1)) - 1  # 0-based como no Pandas
                code = match.group(2)
                length_str = match.group(3).replace('$', '').split('.')[0]
                length = int(length_str)
                positions[code] = (pos, length)
    
    # Aplicar corre√ß√µes
    positions.update(POSITION_CORRECTIONS)
    
    return positions

def clean_and_map_column(df, col_name, mapping):
    """
    Limpa coluna (remove zeros √† esquerda, trim) e aplica mapeamento.
    Replica l√≥gica do Pandas: .strip().lstrip('0').replace('', '0')
    """
    # Trim
    df = df.withColumn(col_name, trim(col(col_name)))
    
    # Remover zeros √† esquerda (mas manter '0' se for s√≥ zeros)
    df = df.withColumn(
        col_name,
        when(regexp_replace(col(col_name), "^0+", "") == "", "0")
        .otherwise(regexp_replace(col(col_name), "^0+", ""))
    )
    
    # Aplicar mapeamento
    mapping_expr = col(col_name)
    for key, value in mapping.items():
        mapping_expr = when(col(col_name) == key, value).otherwise(mapping_expr)
    
    df = df.withColumn(col_name, mapping_expr)
    return df

# ==========================================
# ETAPA 1: PARSE POSI√á√ïES
# ==========================================

print("üìñ [1/6] Parseando posi√ß√µes do SAS...")
positions = parse_sas_positions(SAS_FILE)

print(f"‚úÖ {len(positions)} posi√ß√µes encontradas\n")
print("üìã Posi√ß√µes para colunas desejadas:")
for code in list(DESIRED_COLUMNS.keys())[:10]:
    if code in positions:
        print(f"   {code:15} ‚Üí pos={positions[code][0]:4}, len={positions[code][1]}")
print(f"   ... e mais {len(DESIRED_COLUMNS) - 10}")

# ==========================================
# ETAPA 2: EXTRA√á√ÉO COM SPARK
# ==========================================

print("\nüì• [2/6] Extraindo colunas do arquivo raw...")

# Ler arquivo como texto
df_raw = spark.read.text(str(RAW_FILE), lineSep="\n")
n_total = df_raw.count()
print(f"   ‚úì {n_total:,} registros lidos")

# Extrair apenas colunas desejadas
extract_expr = []
codes_found = []

for code, col_name in DESIRED_COLUMNS.items():
    if code in positions:
        pos, length = positions[code]
        # Substring √© 1-based no Spark, ent√£o pos+1
        extract_expr.append(substring(col("value"), pos + 1, length).alias(col_name))
        codes_found.append(code)
    else:
        print(f"   ‚ö†Ô∏è  {code} n√£o encontrado no SAS")

df_extracted = df_raw.select(*extract_expr)
print(f"   ‚úì {len(extract_expr)} colunas extra√≠das")

# ==========================================
# ETAPA 3: CONVERS√ÉO DE TIPOS
# ==========================================

print("\nüî¢ [3/6] Convertendo tipos de dados...")

# Colunas num√©ricas
numeric_cols = [
    "peso_amostral", "idade", "anos_estudo", "renda_percapita", 
    "num_medicamentos", "peso_real", "altura", "autoavaliacao_saude",
    "internacoes_12m"
]

# Converter num√©ricas
for col_name in numeric_cols:
    if col_name in df_extracted.columns:
        df_extracted = df_extracted.withColumn(
            col_name,
            when(
                trim(col(col_name)).rlike("^[0-9]+$") & (trim(col(col_name)) != ""),
                col(col_name).cast(DoubleType())
            ).otherwise(lit(None))
        )

# Trim em todas as strings
string_cols = [c for c in df_extracted.columns if c not in numeric_cols]
for col_name in string_cols:
    df_extracted = df_extracted.withColumn(col_name, trim(col(col_name)))

print(f"   ‚úì {len(numeric_cols)} colunas num√©ricas convertidas")

# ==========================================
# ETAPA 4: FILTRO 60+ ANOS
# ==========================================

print("\nüéØ [4/6] Filtrando popula√ß√£o 60+ anos...")
df_filtered = df_extracted.filter(col("idade") >= 60)
n_60plus = df_filtered.count()
print(f"   ‚úì {n_total:,} ‚Üí {n_60plus:,} registros ({(n_60plus/n_total)*100:.1f}%)")

# ==========================================
# ETAPA 5: MAPEAMENTOS CATEG√ìRICOS
# ==========================================

print("\nüè∑Ô∏è  [5/6] Aplicando mapeamentos categ√≥ricos...")

mapped_count = 0
for col_name, mapping in MAPPINGS.items():
    if col_name in df_filtered.columns:
        print(f"   Mapeando {col_name}...", end="")
        df_filtered = clean_and_map_column(df_filtered, col_name, mapping)
        mapped_count += 1
        print(" ‚úì")

print(f"   ‚úì {mapped_count} colunas mapeadas")

# ==========================================
# ETAPA 6: VARI√ÅVEIS DERIVADAS
# ==========================================

print("\nüßÆ [6/6] Criando vari√°veis derivadas...")

# Converter para Pandas para opera√ß√µes complexas
print("   Convertendo para Pandas...")
df_pandas = df_filtered.toPandas()

# 1. IMC
print("   [a] Calculando IMC...")
if "peso_real" in df_pandas.columns and "altura" in df_pandas.columns:
    df_pandas["altura_m"] = df_pandas["altura"] / 100  # cm para metros
    df_pandas["imc"] = df_pandas["peso_real"] / (df_pandas["altura_m"] ** 2)
    df_pandas["imc"] = df_pandas["imc"].fillna(df_pandas["imc"].median())
    print(f"      ‚úì IMC calculado (m√©dia: {df_pandas['imc'].mean():.2f})")

# 2. Converter bin√°rios para 0/1
print("   [b] Convertendo vari√°veis bin√°rias...")
binary_cols = [
    "possui_plano_saude", "consulta_12m", "atendimento_sus", "usa_internet",
    "usa_celular", "depressao_diag", "vacina_influenza", "atividade_fisica",
    "fumante_atual", "hipertensao", "diabetes", "doenca_cardiaca", "avc",
    "doenca_respiratoria", "cancer"
]

for col_name in binary_cols:
    if col_name in df_pandas.columns:
        # Converter 'Sim'/'N√£o' para 1/0
        df_pandas[col_name] = df_pandas[col_name].replace({"Sim": 1, "N√£o": 0, "N√£o sabe": 0, "Ignorado": 0})
        df_pandas[col_name] = pd.to_numeric(df_pandas[col_name], errors='coerce').fillna(0).astype(int)

# 3. Multimorbidade
print("   [c] Calculando multimorbidade...")
chronic_cols = ["hipertensao", "diabetes", "doenca_cardiaca", "avc", "doenca_respiratoria", "cancer", "depressao_diag"]
chronic_present = [c for c in chronic_cols if c in df_pandas.columns]

if chronic_present:
    df_pandas["multimorbidade_count"] = df_pandas[chronic_present].sum(axis=1)
    df_pandas["multimorbidade_cat"] = pd.cut(
        df_pandas["multimorbidade_count"],
        bins=[-1, 0, 1, 2, 100],
        labels=["0", "1", "2", "3+"]
    ).astype(str)
    print(f"      ‚úì Multimorbidade (m√©dia: {df_pandas['multimorbidade_count'].mean():.2f})")

# 4. Escores funcionais (ADL/IADL)
print("   [d] Calculando escores funcionais...")
adl_cols = ["dificuldade_vestir", "dificuldade_banho", "dificuldade_alimentar"]
iadl_cols = ["dificuldade_compras", "dificuldade_medico"]

# Converter dificuldades para num√©rico (Nenhuma=0, Alguma=1, Muita=2, N√£o consegue=3)
difficulty_mapping = {"Nenhuma": 0, "Alguma": 1, "Muita": 2, "N√£o consegue": 3, "Ignorado": 0}

for col_name in adl_cols + iadl_cols:
    if col_name in df_pandas.columns:
        df_pandas[col_name] = df_pandas[col_name].replace(difficulty_mapping)
        df_pandas[col_name] = pd.to_numeric(df_pandas[col_name], errors='coerce').fillna(0).astype(int)

adl_present = [c for c in adl_cols if c in df_pandas.columns]
iadl_present = [c for c in iadl_cols if c in df_pandas.columns]

if adl_present:
    df_pandas["adl_score"] = df_pandas[adl_present].sum(axis=1)
    print(f"      ‚úì ADL Score (m√©dia: {df_pandas['adl_score'].mean():.2f})")

if iadl_present:
    df_pandas["iadl_score"] = df_pandas[iadl_present].sum(axis=1)
    print(f"      ‚úì IADL Score (m√©dia: {df_pandas['iadl_score'].mean():.2f})")

# 5. Functional Score normalizado
if adl_present or iadl_present:
    df_pandas["functional_raw"] = 0
    if "adl_score" in df_pandas.columns:
        df_pandas["functional_raw"] += df_pandas["adl_score"]
    if "iadl_score" in df_pandas.columns:
        df_pandas["functional_raw"] += df_pandas["iadl_score"]
    
    max_raw = df_pandas["functional_raw"].max() if df_pandas["functional_raw"].max() > 0 else 1
    df_pandas["functional_score"] = 1 - (df_pandas["functional_raw"] / max_raw)
    print(f"      ‚úì Functional Score (m√©dia: {df_pandas['functional_score'].mean():.3f})")

# 6. Depend√™ncia SUS
print("   [e] Calculando depend√™ncia SUS...")
if "possui_plano_saude" in df_pandas.columns and "atendimento_sus" in df_pandas.columns:
    df_pandas["dependencia_SUS"] = (
        (df_pandas["possui_plano_saude"] == 0) & (df_pandas["atendimento_sus"] == 1)
    ).astype(int)
    print(f"      ‚úì Depend√™ncia SUS ({df_pandas['dependencia_SUS'].sum():,} pessoas, {df_pandas['dependencia_SUS'].mean()*100:.1f}%)")

# 7. Cobertura influenza
if "vacina_influenza" in df_pandas.columns:
    df_pandas["cobertura_influenza"] = df_pandas["vacina_influenza"]

# 8. Health Score composto
print("   [f] Calculando Health Score...")
required_cols = ["autoavaliacao_saude", "multimorbidade_count", "functional_score"]
if all(c in df_pandas.columns for c in required_cols):
    # Converter autoavalia√ß√£o para num√©rico
    health_mapping = {"Muito boa": 1, "Boa": 2, "Regular": 3, "Ruim": 4, "Muito ruim": 5}
    df_pandas["autoav_numeric"] = df_pandas["autoavaliacao_saude"].replace(health_mapping)
    df_pandas["autoav_numeric"] = pd.to_numeric(df_pandas["autoav_numeric"], errors='coerce')
    
    # Z-scores
    df_pandas["autoav_z"] = (df_pandas["autoav_numeric"] - df_pandas["autoav_numeric"].mean()) / df_pandas["autoav_numeric"].std(ddof=0)
    df_pandas["multimorb_z"] = (df_pandas["multimorbidade_count"] - df_pandas["multimorbidade_count"].mean()) / df_pandas["multimorbidade_count"].std(ddof=0)
    df_pandas["functional_z"] = (df_pandas["functional_score"] - df_pandas["functional_score"].mean()) / df_pandas["functional_score"].std(ddof=0)
    
    # Score composto
    df_pandas["health_score_raw"] = (
        (-0.5 * df_pandas["autoav_z"]) +
        (-0.7 * df_pandas["multimorb_z"]) +
        (1.2 * df_pandas["functional_z"])
    )
    
    # Normalizar 0-1
    min_val = df_pandas["health_score_raw"].min()
    max_val = df_pandas["health_score_raw"].max()
    df_pandas["health_score"] = (df_pandas["health_score_raw"] - min_val) / (max_val - min_val + 1e-9)
    print(f"      ‚úì Health Score (m√©dia: {df_pandas['health_score'].mean():.3f})")

# 9. Imputa√ß√£o
print("   [g] Imputando valores faltantes...")
impute_cols = ["num_medicamentos", "idade", "anos_estudo", "renda_percapita"]
for col_name in impute_cols:
    if col_name in df_pandas.columns:
        missing = df_pandas[col_name].isnull().sum()
        if missing > 0:
            df_pandas[col_name] = df_pandas[col_name].fillna(df_pandas[col_name].median())
            print(f"      ‚Ä¢ {col_name}: {missing:,} valores imputados")

print("\n‚úÖ Todas as vari√°veis derivadas criadas!")

# ==========================================
# EXPORTA√á√ÉO FINAL
# ==========================================

print("\n" + "="*70)
print("üíæ EXPORTA√á√ÉO FINAL")
print("="*70)

# Salvar CSV
print(f"\nüìù Salvando {OUTPUT_CSV.name}...")
df_pandas.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')
file_size = OUTPUT_CSV.stat().st_size / 1024**2

print(f"‚úÖ Arquivo salvo com sucesso!")
print(f"   üìÅ Caminho: {OUTPUT_CSV}")
print(f"   üìä Registros: {len(df_pandas):,}")
print(f"   üìã Colunas: {len(df_pandas.columns)}")
print(f"   üíæ Tamanho: {file_size:.1f} MB")

# Salvar metadados
metadata_file = OUTPUT_DIR / "metadados_completo.txt"
with open(metadata_file, 'w', encoding='utf-8') as f:
    f.write("="*70 + "\n")
    f.write("PNS 2019 - DATASET COMPLETO COM VARI√ÅVEIS DERIVADAS\n")
    f.write("="*70 + "\n\n")
    f.write(f"Registros: {len(df_pandas):,}\n")
    f.write(f"Colunas: {len(df_pandas.columns)}\n\n")
    f.write("COLUNAS:\n")
    f.write("-"*70 + "\n")
    for col in df_pandas.columns:
        f.write(f"{col:35} {df_pandas[col].dtype}\n")

print(f"\nüìã Metadados salvos: {metadata_file.name}")

# Resumo estat√≠stico
print("\nüìä RESUMO ESTAT√çSTICO:")
print("-"*70)

summary_vars = {
    "Idade": "idade",
    "Sexo": "sexo",
    "Multimorbidade": "multimorbidade_count",
    "Functional Score": "functional_score",
    "Health Score": "health_score",
    "Depend√™ncia SUS": "dependencia_SUS"
}

for label, var in summary_vars.items():
    if var in df_pandas.columns:
        if df_pandas[var].dtype in ['object', 'category']:
            top_value = df_pandas[var].value_counts().head(1)
            if len(top_value) > 0:
                print(f"{label:20} (categ√≥rica) - Mais comum: {top_value.index[0]} ({top_value.values[0]:,})")
        else:
            mean_val = df_pandas[var].mean()
            median_val = df_pandas[var].median()
            print(f"{label:20} (num√©rica)    - M√©dia: {mean_val:.2f}, Mediana: {median_val:.2f}")

print("\n" + "="*70)
print("üéâ ETL CONCLU√çDO COM SUCESSO!")
print("="*70)
print("\n‚ú® Dataset pronto para an√°lise!")

# Encerrar Spark
spark.stop()
print("\nüîå Spark encerrado")